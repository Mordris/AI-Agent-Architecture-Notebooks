{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WtcUDe9wpZS",
        "outputId": "e1f739e8-2650-45c1-9c25-9844e0c93012"
      },
      "outputs": [],
      "source": [
        "# First, install the necessary libraries for LangChain\n",
        "!pip install langchain langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDhNHlNNwyyV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "except ImportError:\n",
        "    print(\"Not in a Colab environment, assuming API key is set.\")\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHNDDa1ww7lc",
        "outputId": "5b55e3b6-805f-44d9-f5c1-9ebfdce32acb"
      },
      "outputs": [],
      "source": [
        "# 1. The Brain: Initialize the LLM we'll be using\n",
        "# We set temperature=0 to make the output more deterministic and predictable\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "# 2. The Prompting Station: Define the template for our prompt\n",
        "# This template tells the LLM its role and expects a variable named \"topic\"\n",
        "prompt_template = ChatPromptTemplate.from_template(\n",
        "    \"You are a world-class expert on all things tech. \"\n",
        "    \"Explain the concept of '{topic}' in one simple sentence.\"\n",
        ")\n",
        "\n",
        "# 3. The Output Formatting Station: Define the output parser\n",
        "# This parser will take the LLM's complex output object and extract just the content string.\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# 4. The Assembly Line: Chain these components together\n",
        "# The pipe symbol (|) is LangChain's \"LangChain Expression Language\" (LCEL)\n",
        "# It's a clean way to connect the pieces of the chain.\n",
        "# Read it as: The output of the prompt is \"piped\" as input to the llm,\n",
        "# whose output is then \"piped\" to the output_parser.\n",
        "chain = prompt_template | llm | output_parser\n",
        "\n",
        "# 5. Run the chain!\n",
        "# The .invoke() method triggers the chain.\n",
        "# We pass a dictionary with the key matching the variable in our prompt template.\n",
        "topic = \"Cloud Computing\"\n",
        "print(f\"--- Explaining '{topic}' ---\")\n",
        "response = chain.invoke({\"topic\": topic})\n",
        "print(response)\n",
        "\n",
        "topic = \"Quantum Entanglement\"\n",
        "print(f\"\\n--- Explaining '{topic}' ---\")\n",
        "response = chain.invoke({\"topic\": topic})\n",
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
