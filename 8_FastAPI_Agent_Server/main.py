# main.py

import uvicorn
import os
from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from langchain.memory import ConversationBufferWindowMemory
import asyncio

# Set up your OpenAI API key
try:
    from dotenv import load_dotenv
    load_dotenv()
    openai_api_key = os.getenv("OPENAI_API_KEY")
    if not openai_api_key:
        raise ValueError("OpenAI API key is not set in the environment.")
    print("OpenAI API key loaded successfully.")
except Exception as e:
    print(f"Error: {e}")

# Import the agent executor from our agent.py file
from agent import agent_executor

# Initialize the FastAPI app
app = FastAPI(
    title="Auto-Support Agent API",
    description="The main API for the production-ready AI Auto-Support Agent.",
    version="1.1.0", # Bumped version for the new feature
)

# In-memory storage for user chat histories.
# In a production app, this would be a database (e.g., Redis).
chat_histories = {}

# Define our data models
class ChatRequest(BaseModel):
    user_id: str
    message: str

# Define our API endpoints
@app.get("/health", summary="Health Check")
async def health_check():
    return {"status": "ok"}


# --- The Corrected and Final Streaming Endpoint ---
@app.post("/chat/stream", summary="Process a Chat Message with True Token-by-Token Streaming")
async def chat_stream(request: ChatRequest):
    """
    Processes a user's chat message and streams the agent's response back
    using the low-level `astream_events` method for a true token-by-token experience.
    """
    print(f"Received stream request from user: {request.user_id}")
    print(f"Message: {request.message}")

    # Get or create the chat history for the user
    if request.user_id not in chat_histories:
        chat_histories[request.user_id] = ConversationBufferWindowMemory(
            k=5, return_messages=True
        )
    memory = chat_histories[request.user_id]

    async def stream_generator():
        """
        This is an asynchronous generator that will yield tokens as they become available.
        It uses `astream_events` to get the raw event stream from the agent.
        """
        final_response = ""
        
        # Use astream_events for real-time, granular streaming
        async for event in agent_executor.astream_events(
            {
                "input": request.message,
                "chat_history": memory.chat_memory.messages
            },
            version="v1" # Required for this streaming method
        ):
            kind = event["event"]
            
            # We are interested in the tokens generated by the LLM for the final answer
            if kind == "on_chat_model_stream":
                content = event["data"]["chunk"].content
                if content:
                    # Yield the token to the client
                    yield content
                    # Append to the full response for memory
                    final_response += content

        # After the stream is complete, save the full interaction to memory
        print(f"\nFull response for memory: {final_response}")
        memory.save_context({"input": request.message}, {"output": final_response})

    # Return a StreamingResponse, which is what enables the streaming behavior
    return StreamingResponse(stream_generator(), media_type="text/plain")


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)