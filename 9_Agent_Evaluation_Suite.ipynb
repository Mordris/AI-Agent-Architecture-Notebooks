{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhk1YAbbhZQ-",
        "outputId": "08e3da3a-0352-4fa8-9a40-0d28556604d5"
      },
      "outputs": [],
      "source": [
        "# In a new Colab notebook\n",
        "# First, install the necessary libraries\n",
        "!pip install langchain langchain_openai chromadb pypdf langchain-community\n",
        "\n",
        "import os\n",
        "import asyncio\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# Set up your OpenAI API key\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "except ImportError:\n",
        "    print(\"Not in a Colab environment, assuming API key is set.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81GD0jvThhGi"
      },
      "outputs": [],
      "source": [
        "# --- 1. Re-create a simplified version of our agent ---\n",
        "# In a real project, you would import this from your agent.py file.\n",
        "# For this notebook, we'll redefine it here to keep it self-contained.\n",
        "from datetime import datetime\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.agents import tool, create_openai_tools_agent, AgentExecutor\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "# Define the tools\n",
        "@tool\n",
        "def get_current_time(tool_input: str = \"\") -> str:\n",
        "    \"\"\"Returns the current date and time.\"\"\"\n",
        "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "@tool\n",
        "def get_knowledge_from_library(query: str) -> str:\n",
        "    \"\"\"Searches a library about the Transformer architecture and attention.\"\"\"\n",
        "    persist_directory = './chroma_db_rag'\n",
        "    embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "    vector_store = Chroma(persist_directory=persist_directory, embedding_function=embedding_model)\n",
        "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 2}) # Get 2 docs\n",
        "    docs = retriever.get_relevant_documents(query)\n",
        "    return \"\\n---\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "tools = [get_current_time, get_knowledge_from_library]\n",
        "\n",
        "# Create the agent\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant.\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "])\n",
        "agent = create_openai_tools_agent(llm, tools, prompt)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upmHQK7WhlBa"
      },
      "outputs": [],
      "source": [
        "# --- 2. Define the Test Set ---\n",
        "# This is our \"exam\" for the agent. Each item has an input and the expected outcome.\n",
        "test_set = [\n",
        "    {\n",
        "        \"name\": \"Time Inquiry\",\n",
        "        \"input\": \"What time is it right now?\",\n",
        "        \"expected_tool\": \"get_current_time\",\n",
        "        \"expected_keywords\": [str(datetime.now().year), \":\"] # Check for year and a colon\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"RAG Inquiry\",\n",
        "        \"input\": \"Explain self-attention in one sentence.\",\n",
        "        \"expected_tool\": \"get_knowledge_from_library\",\n",
        "        \"expected_keywords\": [\"self-attention\", \"sequence\", \"dependencies\"]\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"No Tool Inquiry\",\n",
        "        \"input\": \"Write a short poem about AI.\",\n",
        "        \"expected_tool\": None, # Expect no tool to be called\n",
        "        \"expected_keywords\": [\"robot\", \"learn\", \"code\", \"mind\"]\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3Z4v845hn5y"
      },
      "outputs": [],
      "source": [
        "# --- 3. The LLM-as-a-Judge Evaluator ---\n",
        "judge_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "async def evaluate_agent_run(test_case: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Runs a single test case against the agent and uses an LLM to judge the result.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Running Test Case: {test_case['name']} ---\")\n",
        "    input_question = test_case[\"input\"]\n",
        "\n",
        "    # Use astream_events to capture the full trace of the agent's run\n",
        "    events = []\n",
        "    async for event in agent_executor.astream_events({\"input\": input_question}, version=\"v1\"):\n",
        "        events.append(event)\n",
        "\n",
        "    # Extract key information from the events stream\n",
        "    final_answer = \"\"\n",
        "    tool_calls = []\n",
        "    for event in events:\n",
        "        if event[\"event\"] == \"on_tool_start\":\n",
        "            tool_calls.append(event[\"name\"])\n",
        "        if event[\"event\"] == \"on_chat_model_stream\":\n",
        "            final_answer += event[\"data\"][\"chunk\"].content\n",
        "\n",
        "    print(f\"Final Answer: {final_answer}\")\n",
        "    print(f\"Tools Called: {tool_calls}\")\n",
        "\n",
        "    # Now, use the LLM to judge the performance\n",
        "    evaluation_prompt = f\"\"\"\n",
        "    You are an expert evaluator for AI agents. Your task is to score an agent's performance\n",
        "    on a given task based on the provided criteria.\n",
        "\n",
        "    TASK:\n",
        "    - User's Question: \"{input_question}\"\n",
        "    - Expected Tool to be Used: {test_case['expected_tool']}\n",
        "    - Expected Keywords in Final Answer: {test_case['expected_keywords']}\n",
        "\n",
        "    AGENT'S PERFORMANCE:\n",
        "    - Tools Actually Called: {tool_calls}\n",
        "    - Final Answer Provided: \"{final_answer}\"\n",
        "\n",
        "    EVALUATION CRITERIA:\n",
        "    1.  **Tool Correctness (Score 0-1):** Score 1 if the agent called the single expected tool, or no tool if none was expected. Score 0 otherwise.\n",
        "    2.  **Answer Relevance (Score 0-1):** Score 1 if the final answer is relevant and directly addresses the user's question. Score 0 otherwise.\n",
        "    3.  **Keyword Presence (Score 0-1):** Score 1 if the final answer contains ALL of the expected keywords. Score 0 otherwise.\n",
        "\n",
        "    Please provide your evaluation as a JSON object with three keys: \"tool_score\", \"answer_score\", \"keyword_score\".\n",
        "    Do not include any other text or explanation.\n",
        "    \"\"\"\n",
        "\n",
        "    judge_response = await judge_llm.ainvoke(evaluation_prompt)\n",
        "\n",
        "    try:\n",
        "        scores = json.loads(judge_response.content)\n",
        "    except:\n",
        "        # Fallback if the LLM doesn't return perfect JSON\n",
        "        scores = {\"tool_score\": 0, \"answer_score\": 0, \"keyword_score\": 0}\n",
        "\n",
        "    print(f\"Evaluation Scores: {scores}\")\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfWtrG2LhqC3"
      },
      "outputs": [],
      "source": [
        "# --- 4. Run the Full Evaluation Suite ---\n",
        "async def run_suite():\n",
        "    import json # Import json for parsing the judge's response\n",
        "    results = {}\n",
        "    for case in test_set:\n",
        "        scores = await evaluate_agent_run(case)\n",
        "        results[case[\"name\"]] = scores\n",
        "\n",
        "    print(\"\\n\\n--- Evaluation Suite Complete ---\")\n",
        "    # Calculate and print average scores\n",
        "    avg_tool_score = sum(res[\"tool_score\"] for res in results.values()) / len(results)\n",
        "    avg_answer_score = sum(res[\"answer_score\"] for res in results.values()) / len(results)\n",
        "    avg_keyword_score = sum(res[\"keyword_score\"] for res in results.values()) / len(results)\n",
        "\n",
        "    print(f\"\\nAverage Tool Correctness Score: {avg_tool_score:.2f}\")\n",
        "    print(f\"Average Answer Relevance Score: {avg_answer_score:.2f}\")\n",
        "    print(f\"Average Keyword Presence Score: {avg_keyword_score:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAT0reKZhtA5",
        "outputId": "7700a0be-7753-498c-c8e3-565a7068346d"
      },
      "outputs": [],
      "source": [
        "# Run the async function\n",
        "# In a notebook, you can run an async function at the top level with await\n",
        "# await run_suite()\n",
        "# Or, use asyncio.run()\n",
        "import asyncio\n",
        "import json\n",
        "# asyncio.run(run_suite())\n",
        "await run_suite()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zc42H1pjcyQ",
        "outputId": "9544b1c4-ac08-4ebc-9c51-404299067b40"
      },
      "outputs": [],
      "source": [
        "# In a new Colab notebook\n",
        "\n",
        "# =================================================================\n",
        "# Cell 1: Setup and Dependencies\n",
        "# =================================================================\n",
        "print(\"--- Installing Dependencies ---\")\n",
        "!pip install langchain langchain_openai chromadb pypdf langchain-community -q\n",
        "\n",
        "import os\n",
        "import asyncio\n",
        "import json\n",
        "import re\n",
        "from typing import List, Dict, Any\n",
        "from datetime import datetime\n",
        "\n",
        "# Set up your OpenAI API key\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "    print(\"OpenAI API key loaded successfully.\")\n",
        "except ImportError:\n",
        "    print(\"Not in a Colab environment, assuming API key is set.\")\n",
        "\n",
        "# =================================================================\n",
        "# Cell 2: Ingestion - Create the Knowledge Base\n",
        "# This cell creates the './chroma_db_rag' directory needed by our RAG tool.\n",
        "# =================================================================\n",
        "print(\"\\n--- Running Ingestion to Create Knowledge Base ---\")\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# 1. Load the document\n",
        "loader = PyPDFLoader(\"https://arxiv.org/pdf/1706.03762.pdf\")\n",
        "documents = loader.load()\n",
        "\n",
        "# 2. Split the document into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "# 3. Embed and Store\n",
        "persist_directory = './chroma_db_rag'\n",
        "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "vector_store = Chroma.from_documents(\n",
        "    documents=chunks,\n",
        "    embedding=embedding_model,\n",
        "    persist_directory=persist_directory\n",
        ")\n",
        "print(\"--- Knowledge Base Created Successfully ---\")\n",
        "\n",
        "\n",
        "# =================================================================\n",
        "# Cell 3: Agent and Evaluation Suite\n",
        "# =================================================================\n",
        "print(\"\\n--- Defining Agent and Evaluation Suite ---\")\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.agents import tool, create_openai_tools_agent, AgentExecutor\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "# --- 1. Define the Tools ---\n",
        "@tool\n",
        "def get_current_time(tool_input: str = \"\") -> str:\n",
        "    \"\"\"Returns the current date and time.\"\"\"\n",
        "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "@tool\n",
        "def get_knowledge_from_library(query: str) -> str:\n",
        "    \"\"\"Searches a library about the Transformer architecture and attention.\"\"\"\n",
        "    try:\n",
        "        persist_directory = './chroma_db_rag'\n",
        "        embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "        vector_store = Chroma(persist_directory=persist_directory, embedding_function=embedding_model)\n",
        "        retriever = vector_store.as_retriever(search_kwargs={\"k\": 2})\n",
        "        docs = retriever.get_relevant_documents(query)\n",
        "        return \"\\n---\\n\".join([doc.page_content for doc in docs])\n",
        "    except Exception as e:\n",
        "        return f\"Error accessing knowledge base: {e}\"\n",
        "\n",
        "tools = [get_current_time, get_knowledge_from_library]\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant.\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "])\n",
        "agent = create_openai_tools_agent(llm, tools, prompt)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=False)\n",
        "\n",
        "# --- 2. Define the Test Set ---\n",
        "test_set = [\n",
        "    {\"name\": \"Time Inquiry\", \"input\": \"What time is it right now?\", \"expected_tool\": \"get_current_time\", \"expected_keywords\": [str(datetime.now().year), \":\"]},\n",
        "    {\"name\": \"RAG Inquiry\", \"input\": \"Explain self-attention in one sentence.\", \"expected_tool\": \"get_knowledge_from_library\", \"expected_keywords\": [\"self-attention\", \"sequence\", \"dependencies\"]},\n",
        "    {\"name\": \"No Tool Inquiry\", \"input\": \"Write a short poem about AI.\", \"expected_tool\": None, \"expected_keywords\": [\"robot\", \"learn\", \"code\", \"mind\"]}\n",
        "]\n",
        "\n",
        "# --- 3. The LLM-as-a-Judge Evaluator ---\n",
        "judge_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "def extract_json_from_string(text: str) -> Dict[str, Any]:\n",
        "    match = re.search(r\"```json\\s*(\\{.*?\\})\\s*```\", text, re.DOTALL)\n",
        "    if match:\n",
        "        json_str = match.group(1)\n",
        "    else:\n",
        "        json_str = text\n",
        "    try:\n",
        "        return json.loads(json_str)\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error: Could not decode JSON from the string: {text}\")\n",
        "        return {}\n",
        "\n",
        "async def evaluate_agent_run(test_case: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    print(f\"\\n--- Running Test Case: {test_case['name']} ---\")\n",
        "    input_question = test_case[\"input\"]\n",
        "    events = []\n",
        "    async for event in agent_executor.astream_events({\"input\": input_question}, version=\"v1\"):\n",
        "        events.append(event)\n",
        "    final_answer, tool_calls = \"\", []\n",
        "    for event in events:\n",
        "        if event[\"event\"] == \"on_tool_start\": tool_calls.append(event[\"name\"])\n",
        "        if event[\"event\"] == \"on_chat_model_stream\": final_answer += event[\"data\"][\"chunk\"].content\n",
        "\n",
        "    print(f\"Final Answer: {final_answer}\")\n",
        "    print(f\"Tools Called: {tool_calls}\")\n",
        "\n",
        "    # --- THIS IS THE FULL, CORRECTED PROMPT ---\n",
        "    evaluation_prompt = f\"\"\"\n",
        "    You are an expert evaluator for AI agents. Your task is to score an agent's performance\n",
        "    on a given task based on the provided criteria.\n",
        "\n",
        "    TASK:\n",
        "    - User's Question: \"{input_question}\"\n",
        "    - Expected Tool to be Used: {test_case['expected_tool']}\n",
        "    - Expected Keywords in Final Answer: {test_case['expected_keywords']}\n",
        "\n",
        "    AGENT'S PERFORMANCE:\n",
        "    - Tools Actually Called: {tool_calls}\n",
        "    - Final Answer Provided: \"{final_answer}\"\n",
        "\n",
        "    EVALUATION CRITERIA:\n",
        "    1.  **Tool Correctness (Score 0-1):** Score 1 if the agent called the single expected tool (or no tool if none was expected). Score 0 otherwise.\n",
        "    2.  **Answer Relevance (Score 0-1):** Score 1 if the final answer is relevant and directly addresses the user's question. Score 0 otherwise.\n",
        "    3.  **Keyword Presence (Score 0-1):** Score 1 if the final answer contains ALL of the expected keywords. Score 0 otherwise.\n",
        "\n",
        "    Please provide your evaluation as a JSON object with three keys: \"tool_score\", \"answer_score\", \"keyword_score\".\n",
        "    Do not include any other text or explanation. Wrap the JSON in a ```json code block.\n",
        "    \"\"\"\n",
        "\n",
        "    judge_response = await judge_llm.ainvoke(evaluation_prompt)\n",
        "    scores = extract_json_from_string(judge_response.content)\n",
        "    final_scores = {\"tool_score\": scores.get(\"tool_score\", 0), \"answer_score\": scores.get(\"answer_score\", 0), \"keyword_score\": scores.get(\"keyword_score\", 0)}\n",
        "    print(f\"Evaluation Scores: {final_scores}\")\n",
        "    return final_scores\n",
        "\n",
        "# --- 4. Run the Full Evaluation Suite ---\n",
        "async def run_suite():\n",
        "    results = {}\n",
        "    for case in test_set:\n",
        "        scores = await evaluate_agent_run(case)\n",
        "        results[case[\"name\"]] = scores\n",
        "\n",
        "    print(\"\\n\\n--- Evaluation Suite Complete ---\")\n",
        "    if not results:\n",
        "        print(\"No results to display.\")\n",
        "        return\n",
        "\n",
        "    avg_tool_score = sum(res[\"tool_score\"] for res in results.values()) / len(results)\n",
        "    avg_answer_score = sum(res[\"answer_score\"] for res in results.values()) / len(results)\n",
        "    avg_keyword_score = sum(res[\"keyword_score\"] for res in results.values()) / len(results)\n",
        "\n",
        "    print(f\"\\nAverage Tool Correctness Score: {avg_tool_score:.2f}\")\n",
        "    print(f\"Average Answer Relevance Score: {avg_answer_score:.2f}\")\n",
        "    print(f\"Average Keyword Presence Score: {avg_keyword_score:.2f}\")\n",
        "\n",
        "# =================================================================\n",
        "# Cell 4: Execute the Suite\n",
        "# =================================================================\n",
        "await run_suite()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
