{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZcX78s9vveg",
        "outputId": "8b16fea6-fede-44bc-d204-e555174cc386"
      },
      "outputs": [],
      "source": [
        "# First, install the necessary libraries\n",
        "!pip install langchain langchain_openai\n",
        "\n",
        "import os\n",
        "from datetime import datetime\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Set up your OpenAI API key\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "except ImportError:\n",
        "    print(\"Not in a Colab environment, assuming API key is set.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMIUSHg2v45P"
      },
      "outputs": [],
      "source": [
        "# --- 1. The Tool ---\n",
        "# An agent's power comes from its tools. Let's define a very simple one.\n",
        "# This is just a standard Python function.\n",
        "def get_current_time(*args, **kwargs):\n",
        "    \"\"\"\n",
        "    A tool that returns the current date and time as a string.\n",
        "    It takes no arguments.\n",
        "    \"\"\"\n",
        "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYvRELH6v7hT"
      },
      "outputs": [],
      "source": [
        "# --- 2. The Agent's \"Brain\" ---\n",
        "# This is the LLM that will do the reasoning.\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUhZWsoOv_J7"
      },
      "outputs": [],
      "source": [
        "# --- 3. The ReAct Prompt ---\n",
        "# This is the most critical piece. We are teaching the LLM how to behave.\n",
        "# We give it instructions and, most importantly, describe the tools it has access to.\n",
        "prompt_template = \"\"\"\n",
        "You are an assistant that has access to a tool for finding the current time.\n",
        "You must answer the user's question.\n",
        "\n",
        "To use a tool, you can use the following format:\n",
        "Thought: I need to use a tool to find the information.\n",
        "Action: [tool_name]\n",
        "\n",
        "The result of the tool will be returned to you in the following format:\n",
        "Observation: [tool_output]\n",
        "\n",
        "If you have enough information to answer the user's question, you must respond with the final answer in the following format:\n",
        "Thought: I have enough information to answer the user's question.\n",
        "Final Answer: [your final answer]\n",
        "\n",
        "Here are the tools you have access to:\n",
        "- get_current_time: A tool that returns the current date and time as a string. It takes no arguments.\n",
        "\n",
        "Let's begin.\n",
        "\n",
        "User Question: What is the current time in words?\n",
        "{scratchpad}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1B9F9yAwCYT",
        "outputId": "169a955e-4774-4e72-a08b-768918d06061"
      },
      "outputs": [],
      "source": [
        "# --- 4. The Manual ReAct Loop ---\n",
        "# We will manually orchestrate the Thought -> Action -> Observation loop.\n",
        "\n",
        "# We'll use a \"scratchpad\" to keep track of the agent's thoughts and observations\n",
        "scratchpad = \"\"\n",
        "max_loops = 5\n",
        "loop_count = 0\n",
        "\n",
        "while loop_count < max_loops:\n",
        "    loop_count += 1\n",
        "    print(f\"--- Loop {loop_count} ---\")\n",
        "\n",
        "    # Construct the full prompt for this loop\n",
        "    full_prompt = prompt_template.format(scratchpad=scratchpad)\n",
        "\n",
        "    # Get the LLM's response\n",
        "    llm_response = llm.invoke(full_prompt).content\n",
        "    print(f\"LLM Response:\\n{llm_response}\")\n",
        "\n",
        "    # Check if the LLM provided a final answer\n",
        "    if \"Final Answer:\" in llm_response:\n",
        "        final_answer = llm_response.split(\"Final Answer:\")[-1].strip()\n",
        "        print(f\"\\n--- Final Answer Found ---\")\n",
        "        print(final_answer)\n",
        "        break\n",
        "\n",
        "    # If not, check if it wants to use a tool\n",
        "    if \"Action:\" in llm_response:\n",
        "        # Parse out the action\n",
        "        action_str = llm_response.split(\"Action:\")[-1].strip()\n",
        "\n",
        "        # For this simple example, we know the only tool is \"get_current_time\"\n",
        "        if action_str == \"get_current_time\":\n",
        "            tool_output = get_current_time()\n",
        "            print(f\"Tool Output: {tool_output}\")\n",
        "\n",
        "            # Update the scratchpad with the new thought, action, and observation\n",
        "            scratchpad += f\"\\n{llm_response}\\nObservation: {tool_output}\"\n",
        "        else:\n",
        "            scratchpad += f\"\\nInvalid tool '{action_str}' requested.\"\n",
        "            print(f\"Invalid tool '{action_str}' requested.\")\n",
        "\n",
        "    else:\n",
        "        print(\"LLM did not provide a valid action or final answer.\")\n",
        "        break"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
