{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G05NGVG0C7HN"
      },
      "source": [
        "# The Evolution of LangChain Agents: From Classic ReAct to Modern Tool Calling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beJdVvzNEXJR"
      },
      "source": [
        "This notebook is a deep dive into the practical realities of building AI Agents with LangChain. It documents the evolution of agent creation, starting from the foundational, text-based ReAct framework and culminating in the modern, robust, and reliable Tool Calling architecture.\n",
        "\n",
        "</br>\n",
        "The central challenge we explore is a common one for AI engineers: how do we get reliable, well-structured, and debuggable output from an LLM that is fundamentally probabilistic? This notebook is a hands-on journey through that very problem.\n",
        "\n",
        "</br>\n",
        "We will build the same simple \"time-telling\" agent in three distinct stages, each in its own cell:\n",
        "\n",
        "</br>\n",
        "\n",
        "1. **The Classic create_react_agent**: We begin with the original, foundational method for building agents in LangChain. By setting verbose=True, we will immediately see the power of the ReAct reasoning loop, but also witness its primary weakness: the messy, sometimes inconsistent, text-based output that can be difficult to parse and debug.\n",
        "2. **The Engineering Fix (Callbacks & Custom Runners)**: Faced with the messy output from the classic agent, our next step is a common engineering impulse: to try and fix the problem with more code. This cell demonstrates an attempt to wrestle control over the output formatting by using custom LangChain callbacks. This is a valuable exercise in understanding the framework's internals, but it ultimately shows the difficulty of patching a system that is inherently text-based.\n",
        "3. **The Modern Solution (create_openai_tools_agent)**: Finally, we arrive at the absolute fix. This cell demonstrates the modern, industry-standard approach using native Tool Calling. Instead of parsing text, this method relies on the LLM generating a structured JSON object to call tools. The result is a dramatically cleaner, 100% reliable, and easily debuggable agent. The verbose=True output from this agent is a model of clarity.\n",
        "</br>\n",
        "By comparing these three approaches, this notebook tells a critical story about AI engineering: true robustness comes not from patching old methods, but from understanding and adopting the better, more reliable underlying architecture when it becomes available.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cm5piDls3RYf",
        "outputId": "ccbabe62-e685-4261-a5cb-284dd54b3595"
      },
      "outputs": [],
      "source": [
        "# First, install the necessary libraries\n",
        "!pip install langchain langchain_openai\n",
        "\n",
        "import os\n",
        "from datetime import datetime\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.agents import tool, create_react_agent, AgentExecutor\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# Set up your OpenAI API key\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "except ImportError:\n",
        "    print(\"Not in a Colab environment, assuming API key is set.\")\n",
        "\n",
        "# --- 1. Define the Tools ---\n",
        "# LangChain provides a simple `@tool` decorator to turn any Python function\n",
        "# into a tool that the agent can use.\n",
        "# The function's docstring is CRITICAL. The LLM uses the docstring to decide\n",
        "# whether to use the tool and how to use it.\n",
        "@tool\n",
        "def get_current_time(tool_input: str = \"\") -> str:\n",
        "    \"\"\"\n",
        "    Returns the current date and time as a string.\n",
        "    Use this tool when you need to know the current time.\n",
        "    It takes no arguments.\n",
        "    \"\"\"\n",
        "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "# Now, we put all our tools into a list.\n",
        "tools = [get_current_time]\n",
        "\n",
        "# --- 2. Create the Agent ---\n",
        "# LangChain has a default ReAct prompt template, so we don't need to write our own.\n",
        "# We just need to provide the variables it expects: `tools`, `tool_names`, and `input`.\n",
        "\n",
        "# Get the default ReAct prompt template. You can view it to see its structure.\n",
        "# You can find the prompt here: https://smith.langchain.com/hub/hwchase17/react\n",
        "prompt = PromptTemplate.from_template(\"\"\"\n",
        "Answer the following questions as best you can. You have access to the following tools:\n",
        "\n",
        "{tools}\n",
        "\n",
        "Use the following format:\n",
        "\n",
        "Question: the input question you must answer\n",
        "Thought: you should always think about what to do\n",
        "Action: the action to take, should be one of [{tool_names}]\n",
        "Action Input: the input to the action\n",
        "Observation: the result of the action\n",
        "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
        "Thought: I now know the final answer\n",
        "Final Answer: the final answer to the original input question\n",
        "\n",
        "Begin!\n",
        "\n",
        "Question: {input}\n",
        "Thought:{agent_scratchpad}\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "# Initialize the LLM\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "# Create the agent. This binds the LLM to the prompt and instructs it on how to use tools.\n",
        "agent = create_react_agent(llm, tools, prompt)\n",
        "\n",
        "# --- 3. Create the Agent Executor ---\n",
        "# The AgentExecutor is the runtime for the agent. It's what automates the loop we built manually.\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=agent,\n",
        "    tools=tools,\n",
        "    verbose=True  # Set to True to see the agent's reasoning process, just like our manual loop!\n",
        ")\n",
        "\n",
        "# --- 4. Run the Agent! ---\n",
        "user_question = \"What is the current time in words?\"\n",
        "response = agent_executor.invoke({\"input\": user_question})\n",
        "\n",
        "print(\"\\n--- Final Response ---\")\n",
        "print(response[\"output\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_g0TtM6A8wuo",
        "outputId": "3f97e604-3534-4932-c903-fab60cac6a30"
      },
      "outputs": [],
      "source": [
        "# First, install the necessary libraries\n",
        "!pip install langchain langchain_openai\n",
        "\n",
        "import os\n",
        "from datetime import datetime\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.agents import create_react_agent, AgentExecutor\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.tools import Tool\n",
        "import re\n",
        "\n",
        "# Set up your OpenAI API key\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "except ImportError:\n",
        "    print(\"Not in a Colab environment, assuming API key is set.\")\n",
        "\n",
        "# --- 1. Define the Tools ---\n",
        "def get_current_time_function(tool_input=\"\") -> str:\n",
        "    \"\"\"\n",
        "    Returns the current date and time as a string.\n",
        "    Use this tool when you need to know the current time.\n",
        "    This tool takes no arguments.\n",
        "    \"\"\"\n",
        "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "# Create the tool\n",
        "get_current_time_tool = Tool(\n",
        "    name=\"get_current_time\",\n",
        "    func=get_current_time_function,\n",
        "    description=\"Returns the current date and time as a string. Use this tool when you need to know the current time. This tool takes no arguments.\"\n",
        ")\n",
        "\n",
        "# Put all tools into a list\n",
        "tools = [get_current_time_tool]\n",
        "\n",
        "# --- 2. Initialize the LLM ---\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "# --- 3. Define the ReAct prompt template ---\n",
        "prompt = PromptTemplate.from_template(\"\"\"\n",
        "Answer the following questions as best you can. You have access to the following tools:\n",
        "\n",
        "{tools}\n",
        "\n",
        "Use the following format:\n",
        "\n",
        "Question: the input question you must answer\n",
        "Thought: you should always think about what to do\n",
        "Action: the action to take, should be one of [{tool_names}]\n",
        "Action Input: the input to the action\n",
        "Observation: the result of the action\n",
        "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
        "Thought: I now know the final answer\n",
        "Final Answer: the final answer to the original input question\n",
        "\n",
        "Begin!\n",
        "\n",
        "Question: {input}\n",
        "Thought:{agent_scratchpad}\n",
        "\"\"\")\n",
        "\n",
        "# --- 4. Create the agent ---\n",
        "agent = create_react_agent(llm, tools, prompt)\n",
        "\n",
        "# --- 5. Create the AgentExecutor with verbose=False to avoid double output ---\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=agent,\n",
        "    tools=tools,\n",
        "    verbose=False,  # Turn off verbose to control output manually\n",
        "    handle_parsing_errors=True,\n",
        "    max_iterations=10\n",
        ")\n",
        "\n",
        "# --- 6. Test the function directly first ---\n",
        "print(\"=\"*50)\n",
        "print(\"    TESTING FUNCTION DIRECTLY\")\n",
        "print(\"=\"*50)\n",
        "direct_result = get_current_time_function()\n",
        "print(f\"✓ Direct result: {direct_result}\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# --- 7. Custom agent runner with proper formatting ---\n",
        "def run_agent_with_custom_format(agent_executor, user_input):\n",
        "    print(\"\\n--- Running Agent ---\")\n",
        "    print(\"> Entering new AgentExecutor chain...\")\n",
        "\n",
        "    # Get the response\n",
        "    response = agent_executor.invoke({\"input\": user_input})\n",
        "\n",
        "    # Parse the intermediate steps from the response\n",
        "    if 'intermediate_steps' in response:\n",
        "        for step in response['intermediate_steps']:\n",
        "            action, observation = step\n",
        "            print(f\"***Action: {action.tool}***\")\n",
        "            print(f\"***Action Input: {repr(action.tool_input)}***\")\n",
        "            print(f\"***Action Output: {observation}***\")\n",
        "    else:\n",
        "        # Fallback: manually simulate the steps for demonstration\n",
        "        print(\"***Action: get_current_time***\")\n",
        "        print(\"***Action Input: ''***\")\n",
        "        current_time = get_current_time_function()\n",
        "        print(f\"***Action Output: {current_time}***\")\n",
        "\n",
        "    print(f\"Final Answer: {response['output']}\")\n",
        "    print(\"> Finished chain.\")\n",
        "\n",
        "    return response\n",
        "\n",
        "# --- 8. Alternative approach using a custom callback ---\n",
        "from langchain_core.callbacks import BaseCallbackHandler\n",
        "from typing import Any, Dict, List\n",
        "\n",
        "class CustomFormatCallback(BaseCallbackHandler):\n",
        "    def on_tool_start(self, serialized: Dict[str, Any], input_str: str, **kwargs: Any) -> None:\n",
        "        tool_name = serialized.get(\"name\", \"unknown\")\n",
        "        print(f\"***Action: {tool_name}***\")\n",
        "        print(f\"***Action Input: {repr(input_str)}***\")\n",
        "\n",
        "    def on_tool_end(self, output: str, **kwargs: Any) -> None:\n",
        "        print(f\"***Action Output: {output}***\")\n",
        "        print()  # Add blank line after action output\n",
        "\n",
        "# --- 9. Run the agent with custom callback ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"    RUNNING LANGCHAIN REACT AGENT\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "callback_handler = CustomFormatCallback()\n",
        "\n",
        "try:\n",
        "    response = agent_executor.invoke(\n",
        "        {\"input\": \"What is the current time in words?\"},\n",
        "        {\"callbacks\": [callback_handler]}\n",
        "    )\n",
        "    print(\"Final Answer:\", response['output'])\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Callback approach failed: {e}\")\n",
        "    print(\"Using fallback method...\")\n",
        "\n",
        "    # Fallback to custom runner\n",
        "    response = run_agent_with_custom_format(agent_executor, \"What is the current time in words?\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*30)\n",
        "print(\"   FINAL RESPONSE\")\n",
        "print(\"-\"*30)\n",
        "print(response[\"output\"])\n",
        "print(\"-\"*30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xV6zvWgjCH4v",
        "outputId": "b9dc9a91-00ca-45a9-cd4e-f0ee568be418"
      },
      "outputs": [],
      "source": [
        "# First, install the necessary libraries\n",
        "!pip install langchain langchain_openai\n",
        "\n",
        "import os\n",
        "from datetime import datetime\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.agents import tool, create_openai_tools_agent, AgentExecutor\n",
        "from langchain import hub # We'll pull a modern prompt from the hub\n",
        "\n",
        "# Set up your OpenAI API key\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "except ImportError:\n",
        "    print(\"Not in a Colab environment, assuming API key is set.\")\n",
        "\n",
        "# --- 1. Define the Tools ---\n",
        "# The tool definition is the same. The docstring remains critical.\n",
        "@tool\n",
        "def get_current_time(tool_input: str = \"\") -> str:\n",
        "    \"\"\"\n",
        "    Returns the current date and time as a string.\n",
        "    Use this tool for any questions about the current time.\n",
        "    \"\"\"\n",
        "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "tools = [get_current_time]\n",
        "\n",
        "# --- 2. Create the Modern \"Tool Calling\" Agent ---\n",
        "\n",
        "# Instead of writing our own ReAct prompt, we pull a pre-built, modern prompt\n",
        "# from LangChain Hub. This prompt is specifically designed for OpenAI's Tool Calling feature.\n",
        "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
        "\n",
        "# Let's inspect the prompt. It's much simpler! It doesn't have the complex\n",
        "# \"Action: Action Input:\" formatting instructions.\n",
        "print(\"--- Modern Agent Prompt ---\")\n",
        "print(prompt.messages[0].prompt.template)\n",
        "print(\"---------------------------\\n\")\n",
        "\n",
        "# Initialize the LLM\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "# Create the agent using the MODERN constructor.\n",
        "# This constructor is specifically for models that support native tool calling.\n",
        "agent = create_openai_tools_agent(llm, tools, prompt)\n",
        "\n",
        "# --- 3. Create the Agent Executor ---\n",
        "# The executor is created in the same way, but it will behave differently under the hood.\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=agent,\n",
        "    tools=tools,\n",
        "    verbose=True # Now, verbose=True will show clean, structured tool calls!\n",
        ")\n",
        "\n",
        "# --- 4. Run the Agent! ---\n",
        "user_question = \"What is the current time in words?\"\n",
        "response = agent_executor.invoke({\"input\": user_question})\n",
        "\n",
        "print(\"\\n--- Final Response ---\")\n",
        "print(response[\"output\"])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
